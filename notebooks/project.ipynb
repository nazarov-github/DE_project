{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a860edb-16e8-4a5d-9393-178f9620c238",
   "metadata": {},
   "source": [
    "# Импорт необходимых библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba897f11-ee68-4db4-9f2b-ae2e5d49d899",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import boto3\n",
    "from pyspark.sql import SparkSession\n",
    "import requests\n",
    "import logging\n",
    "from airflow.models import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b24000b-96e6-4e42-b1ed-41b9c05729b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_api_data():\n",
    "    response = requests.get(\"https://api.open-meteo.com/v1/forecast\",\n",
    "    params={\n",
    "            \"latitude\": 55.75,\n",
    "            \"longitude\": 37.62,\n",
    "            \"current_weather\": True\n",
    "            }\n",
    "                           )\n",
    "    response.raise_for_status()\n",
    "    data = response.json()\n",
    "    return data\n",
    "\n",
    "#get_api_data()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444ca611-c395-4dc2-b68c-b9fe91ce9312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json_to_s3():\n",
    "    s3 = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url=f\"http://{Variable.get(\"MINIO_ENDPOINT\")\",\n",
    "    aws_access_key_id=Variable.get(\"MINIO_ACCESS_KEY\"),\n",
    "    aws_secret_access_key=Variable.get(\"MINIO_SECRET_KEY\")\n",
    "    )\n",
    "    \n",
    "    data = get_api_data()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba028e42-2da6-46d0-ad22-a4f15f05c2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "MINIO_ENDPOINT=minio:9000\n",
    "MINIO_ACCESS_KEY=minio\n",
    "MINIO_SECRET_KEY=minio123\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69974b4c-e30d-4925-9a5b-67025ed94dd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c241391-9ec4-49a3-bc25-a70038f1b782",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869ee09e-f1a5-44c3-ac49-c536dad24d50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debe8f56-64be-4cd5-a5a5-61f6b3d1c62b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338b6e1a-a24a-4b0e-8fa9-5a981de1e111",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac42b28b-c5d0-428e-9c65-1f924c383079",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a630a56b-c0fe-46c5-9126-3128cab6efab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a709dc3-a28f-4449-a64d-022e61ccbe94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8a31068-a2f1-40e3-8f53-275765e1d9c3",
   "metadata": {},
   "source": [
    "# Подключение к Minio через библиотеку boto3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "778ad097-c3b4-4786-8e2f-a3674d2276fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Подключение к MinIO через boto3 успешно. Список бакетов:\n",
      "  - weather\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# Получаем переменные\n",
    "endpoint = os.getenv(\"MINIO_ENDPOINT\") # для airflow нужно будет переписать чтобы значения брались из Variables\n",
    "access_key = os.getenv(\"MINIO_ACCESS_KEY\")\n",
    "secret_key = os.getenv(\"MINIO_SECRET_KEY\")\n",
    "\n",
    "# Подключаемся к MinIO\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url=f\"http://{endpoint}\",\n",
    "    aws_access_key_id=access_key,\n",
    "    aws_secret_access_key=secret_key\n",
    "\n",
    ")\n",
    "\n",
    "# Проверяем список бакетов\n",
    "try:\n",
    "    buckets = s3.list_buckets()\n",
    "    print(\"Подключение к MinIO через boto3 успешно. Список бакетов:\")\n",
    "    for b in buckets['Buckets']:\n",
    "        print(f\"  - {b['Name']}\")\n",
    "except Exception as e:\n",
    "    print(\"Ошибка подключения к MinIO:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a018153f-9ed6-4bb5-9e02-d615bba896a8",
   "metadata": {},
   "source": [
    "# **Загрузка через boto3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f51dc4-94eb-439b-84f4-ca3575858118",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.hooks.base import BaseHook\n",
    "import boto3\n",
    "\n",
    "def upload_to_minio():\n",
    "    conn = BaseHook.get_connection('minio_default')\n",
    "    extra = conn.extra_dejson  # Параметры из Extra\n",
    "    \n",
    "    s3 = boto3.client(\n",
    "        's3',\n",
    "        endpoint_url=extra['endpoint_url'],\n",
    "        aws_access_key_id=extra['aws_access_key_id'],\n",
    "        aws_secret_access_key=extra['aws_secret_access_key']\n",
    "    )\n",
    "    \n",
    "    s3.upload_file('/local/path/file.txt', 'bucket-name', 'file.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e374b0-3430-4a9d-8c2e-aaa68b1cda4c",
   "metadata": {},
   "source": [
    "# Подключение к Minio через библиотеку Minio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2b56e1bd-7a84-4148-babc-e89c68855023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Подключение к MinIO через MinIO успешно. Список бакетов:\n",
      " - project\n"
     ]
    }
   ],
   "source": [
    "client = Minio(\n",
    "    endpoint=os.getenv(\"MINIO_ENDPOINT\"), # для airflow нужно будет переписать чтобы значения брались из Variables\n",
    "    access_key=os.getenv(\"MINIO_ACCESS_KEY\"),\n",
    "    secret_key=os.getenv(\"MINIO_SECRET_KEY\"),\n",
    "    secure=False  \n",
    ")\n",
    "\n",
    "# Проверяем список бакетов\n",
    "try:\n",
    "    buckets = client.list_buckets()\n",
    "    print(\"Подключение к MinIO через MinIO успешно. Список бакетов:\")\n",
    "    for b in buckets:\n",
    "        print(f' - {b.name}')\n",
    "except Exception as e:\n",
    "    print(\"Ошибка подключения к MinIO:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f418fbc2-8e36-4700-93fd-b1fcc2587642",
   "metadata": {},
   "source": [
    "# Проверка SparkSession "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "aaf8eb5c-412d-4314-8517-dbc3e853b605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MinIO Test\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.range(10)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abad1ce4-3e3d-4c6a-a330-7b59e312f1d6",
   "metadata": {},
   "source": [
    "# Запуск SparkSession с различными параметрами "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053e4401-70af-4e95-aef3-d3b55b2aee7f",
   "metadata": {},
   "source": [
    "**Имя приложения, которое будет отражено в Spark UI и логах**\n",
    "\n",
    "appName(\"SparkProject\")\n",
    "\n",
    "**Локальный режим (может быть yarn, mesos, kubernetes)**\n",
    "\n",
    "master(\"local[*]\") \n",
    "\n",
    "**Количество партиций, используемых при операциях shuffle (например, groupBy, join). По умолчанию = 200.\n",
    "Для небольших наборов данных можно уменьшить, чтобы не было лишних партиций.**\n",
    "\n",
    "config(\"spark.sql.shuffle.partitions\", \"200\") \n",
    "\n",
    "**Объем памяти, выделяемый каждому executor'у. Чем больше данных — тем больше памяти потребуется.**\n",
    "\n",
    "config(\"spark.executor.memory\", \"4g\")\n",
    "\n",
    "**Объем памяти для драйвера — основной управляющий процесс, запускающий задачи и отслеживающий их выполнение.**\n",
    "\n",
    "config(\"spark.driver.memory\", \"2g\") \n",
    "\n",
    "**Количество ядер CPU, которые каждый executor может использовать. Больше ядер — больше параллелизма, но стоит учитывать общую нагрузку.**\n",
    "\n",
    "config(\"spark.executor.cores\", \"4\")\n",
    "\n",
    "**Включает динамическое выделение executors в зависимости от загрузки. Позволяет Spark автоматически масштабироваться: добавлять и убирать executors в зависимости от нужд приложения. Требует настройки shuffle service.**\n",
    "\n",
    "config(\"spark.dynamicAllocation.enabled\", \"true\")\n",
    "\n",
    "**Указывает директорию для checkpoint'ов (резервного сохранения состояния, особенно в streaming-приложениях).**\n",
    "\n",
    "config(\"spark.checkpoint.dir\", \"/path/to/checkpoint/dir\")\n",
    "\n",
    "**Путь к директории, где Spark SQL будет хранить управляемые таблицы (при использовании saveAsTable, например).**\n",
    "\n",
    "config(\"spark.sql.catalogImplementation\", \"hive\")\n",
    "\n",
    "\n",
    "**Указывает, что Spark должен использовать Hive Catalog (а не встроенный In-Memory метастор). Требует наличие Hive или его имитации (например, Derby DB).**\n",
    "\n",
    ".enableHiveSupport()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a4f7ec5-ab03-4eb3-8bb7-00ebd837675b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"SparkProject\") \\ \n",
    "#     .master(\"local[*]\") \\ \n",
    "#     .config(\"spark.sql.shuffle.partitions\", \"200\") \\ \n",
    "#     .config(\"spark.executor.memory\", \"4g\") \\ \n",
    "#     .config(\"spark.driver.memory\", \"2g\") \\ \n",
    "#     .config(\"spark.executor.cores\", \"4\") \\ \n",
    "#     .config(\"spark.dynamicAllocation.enabled\", \"true\") \\ \n",
    "#     .config(\"spark.checkpoint.dir\", \"/path/to/checkpoint/dir\") \\ \n",
    "#     .config(\"spark.sql.warehouse.dir\", \"/path/to/warehouse/dir\") \\ \n",
    "#     .config(\"spark.sql.catalogImplementation\", \"hive\") \\ \n",
    "#     .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03561232-96d9-4476-81fd-d6525024dbb3",
   "metadata": {},
   "source": [
    "# Получаем данные по API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdbe37b9-b575-4a41-8f97-5795624187d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_api_data(env_name):\n",
    "    load_dotenv()\n",
    "    # Получаем ссылку из файла .env\n",
    "    api_url = os.getenv(env_name) # для airflow нужно будет переписать чтобы значения брались из Variables\n",
    "    # Проверка наличия ссылки\n",
    "    if not api_url:\n",
    "        logging.error(f'Указанный URL не задан в файле .env') # для airflow нужно будет переписать чтобы значения брались из Variables\n",
    "        return None\n",
    "    try:\n",
    "        response = requests.get(api_url)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            json_str = json.dumps(data, ensure_ascii=False, indent=2)\n",
    "            logging.info('Данные успешно загружены')\n",
    "            return json_str\n",
    "        else:\n",
    "            logging.error(f'Error: {response.status_code} - Данные не загружены')\n",
    "            return None\n",
    "    except:\n",
    "        logging.error(f'Error{e} - Данные не загружены')\n",
    "        raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6287e45e-c426-4b26-8c1e-6ac79a4a37c2",
   "metadata": {},
   "source": [
    "# Сохраняем JSON в MinIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e36b1b0-3a4f-48f6-920a-dd8fbe86a51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json_to_minio(env_name):\n",
    "    data = get_api_data(env_name)\n",
    "    \n",
    "# Написать функцию, чтобы записать в Minio JSON\n",
    "# Проверить в какой бакет пишем перезаписью\n",
    "# Через Spark читать файл\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a295514d-7ef2-47b5-b48f-9a439039b6c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390dfedf-7d9d-418c-9f1a-6fb6ba2c8937",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkProject\") \\ \n",
    "    .master(\"local[*]\") \\ \n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\ \n",
    "    .config(\"spark.driver.memory\", \"2g\") \\ \n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\") \\ \n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5a9986-9252-4f28-8a73-972086cd03b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d27737-e50e-4100-aa07-bc9071ca778b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fb8478-b0db-4296-b5cb-f9a357a1a203",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3879c2d3-a701-4b42-873b-e5431454b6e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d888304-470e-4530-8a9c-fe1662a929b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
