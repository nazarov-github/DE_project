{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a860edb-16e8-4a5d-9393-178f9620c238",
   "metadata": {},
   "source": [
    "# Импорт необходимых библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba897f11-ee68-4db4-9f2b-ae2e5d49d899",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, SQLContext\n",
    "import pyspark.sql.functions as F\n",
    "import logging\n",
    "import os\n",
    "from airflow.models import Variable\n",
    "from dotenv import load_dotenv\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DataType\n",
    "import requests\n",
    "import json\n",
    "import boto3\n",
    "import minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee0423e-e60a-489f-a8b3-196a388bdd0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b24000b-96e6-4e42-b1ed-41b9c05729b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "\n",
    "# Загрузка и проверка переменных окружения\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "\n",
    "def create_spark_session():\n",
    "    try:\n",
    "        spark = SparkSession.builder \\\n",
    "            .appName(\"MinIO Data Reader\") \\\n",
    "            .config(\"spark.hadoop.fs.s3a.access.key\", \"app_user\") \\\n",
    "            .config(\"spark.hadoop.fs.s3a.secret.key\", \"secure_password123\") \\\n",
    "            .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "            .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "            .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "            .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "            .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "            .config(\"spark.jars\", \"/opt/jars/hadoop-aws-3.3.1.jar,/opt/jars/aws-java-sdk-bundle-1.11.901.jar\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "    \n",
    "        logging.info('SparkSession успешно создана для работы с MinIO')\n",
    "    except Exception as e:\n",
    "        logging.error(f'SparkSession не создана по причине: {e}')\n",
    "        raise\n",
    "    return spark\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dba2e8-25b3-408e-89d6-78319c5ed103",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "444ca611-c395-4dc2-b68c-b9fe91ce9312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\u001b[34m2025-07-11T20:09:15.610+0000\u001b[0m] {\u001b[34m4075142977.py:\u001b[0m26} INFO\u001b[0m - SparkSession успешно создана для работы с MinIO\u001b[0m\n",
      "+--------+---------+--------+-----------+----------------+-----------+---------+\n",
      "|latitude|longitude|timezone|format_time|            time|temperature|windspeed|\n",
      "+--------+---------+--------+-----------+----------------+-----------+---------+\n",
      "|   55.75|   37.625|     GMT|    iso8601|2025-07-11T20:00|       27.5|      3.9|\n",
      "+--------+---------+--------+-----------+----------------+-----------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[latitude: double, longitude: double, timezone: string, format_time: string, time: string, temperature: double, windspeed: double]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def download_to_clickhouse():\n",
    "    path = \"s3a://weather/weather/weather_data.json\"\n",
    "    spark = create_spark_session()\n",
    "    schema = StructType([\n",
    "        StructField(\"latitude\", DoubleType()),\n",
    "        StructField(\"longitude\", DoubleType()),\n",
    "        StructField(\"timezone\", StringType()),\n",
    "        StructField(\"current_weather_units\", StructType([\n",
    "            StructField(\"time\", StringType())\n",
    "        ])),\n",
    "        StructField(\"current_weather\", StructType([\n",
    "            StructField(\"time\", StringType()),\n",
    "            StructField(\"temperature\", DoubleType()),\n",
    "            StructField(\"windspeed\", DoubleType())\n",
    "    ]))\n",
    "    ])\n",
    "    raw_df = spark.read.json(path, schema=schema, multiLine=True)\n",
    "    #raw_df.show(truncate=False)\n",
    "    #raw_df.printSchema()\n",
    "    flat_df = raw_df.select(\n",
    "        \"latitude\", \"longitude\", \"timezone\",\n",
    "        F.col(\"current_weather_units.time\").alias(\"format_time\"),\n",
    "        F.col(\"current_weather.time\").alias(\"time\"),\n",
    "        F.col(\"current_weather.temperature\").alias(\"temperature\"),\n",
    "        F.col(\"current_weather.windspeed\").alias(\"windspeed\")\n",
    "        \n",
    "    )\n",
    "    flat_df.show()\n",
    "    return flat_df\n",
    "download_to_clickhouse()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba028e42-2da6-46d0-ad22-a4f15f05c2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "try:\n",
    "    response = requests.get('http://minio:9000')\n",
    "    print('Response status:', response.status_code)\n",
    "except Exception as e:\n",
    "    print('Request error:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69974b4c-e30d-4925-9a5b-67025ed94dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_api_data():\n",
    "    try:\n",
    "        response = requests.get(\"https://api.open-meteo.com/v1/forecast\",\n",
    "        params={\n",
    "                \"latitude\": 55.75,\n",
    "                \"longitude\": 37.62,\n",
    "                \"current_weather\": True\n",
    "                }\n",
    "        )\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            json_str = json.dumps(data, ensure_ascii=False, indent=2)\n",
    "            logging.info('Данные успешно загружены')\n",
    "            return json_str\n",
    "        else:\n",
    "            logging.error(f'Error: {response.status_code} - Данные не загружены')\n",
    "            return None\n",
    "    except:\n",
    "        logging.error(f'Error: Данные не загружены')\n",
    "        raise\n",
    "\n",
    "get_api_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c241391-9ec4-49a3-bc25-a70038f1b782",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "869ee09e-f1a5-44c3-ac49-c536dad24d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession создана\n",
      "Подключенные JAR-файлы: /home/jovyan/jars/hadoop-aws-3.3.1.jar,/home/jovyan/jars/aws-java-sdk-bundle-1.11.901.jar,/home/jovyan/jars/clickhouse-jdbc-0.3.2.jar\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "jars = [\n",
    "    \"/home/jovyan/jars/hadoop-aws-3.3.1.jar\",\n",
    "    \"/home/jovyan/jars/aws-java-sdk-bundle-1.11.901.jar\",\n",
    "    \"/home/jovyan/jars/clickhouse-jdbc-0.3.2.jar\"\n",
    "]\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Test SparkSession\") \\\n",
    "    .config(\"spark.jars\", \",\".join(jars)) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"SparkSession создана\")\n",
    "print(\"Подключенные JAR-файлы:\", spark.sparkContext.getConf().get(\"spark.jars\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debe8f56-64be-4cd5-a5a5-61f6b3d1c62b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338b6e1a-a24a-4b0e-8fa9-5a981de1e111",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac42b28b-c5d0-428e-9c65-1f924c383079",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a630a56b-c0fe-46c5-9126-3128cab6efab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a709dc3-a28f-4449-a64d-022e61ccbe94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8a31068-a2f1-40e3-8f53-275765e1d9c3",
   "metadata": {},
   "source": [
    "# Подключение к Minio через библиотеку boto3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778ad097-c3b4-4786-8e2f-a3674d2276fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# Получаем переменные\n",
    "endpoint = os.getenv(\"MINIO_ENDPOINT\") # для airflow нужно будет переписать чтобы значения брались из Variables\n",
    "access_key = os.getenv(\"MINIO_ACCESS_KEY\")\n",
    "secret_key = os.getenv(\"MINIO_SECRET_KEY\")\n",
    "\n",
    "# Подключаемся к MinIO\n",
    "s3 = boto3.client(\n",
    "    's3',\n",
    "    endpoint_url=f\"http://{endpoint}\",\n",
    "    aws_access_key_id=access_key,\n",
    "    aws_secret_access_key=secret_key\n",
    "\n",
    ")\n",
    "\n",
    "# Проверяем список бакетов\n",
    "try:\n",
    "    buckets = s3.list_buckets()\n",
    "    print(\"Подключение к MinIO через boto3 успешно. Список бакетов:\")\n",
    "    for b in buckets['Buckets']:\n",
    "        print(f\"  - {b['Name']}\")\n",
    "except Exception as e:\n",
    "    print(\"Ошибка подключения к MinIO:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a018153f-9ed6-4bb5-9e02-d615bba896a8",
   "metadata": {},
   "source": [
    "# **Загрузка через boto3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f51dc4-94eb-439b-84f4-ca3575858118",
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.hooks.base import BaseHook\n",
    "import boto3\n",
    "\n",
    "def upload_to_minio():\n",
    "    conn = BaseHook.get_connection('minio_default')\n",
    "    extra = conn.extra_dejson  # Параметры из Extra\n",
    "    \n",
    "    s3 = boto3.client(\n",
    "        's3',\n",
    "        endpoint_url=extra['endpoint_url'],\n",
    "        aws_access_key_id=extra['aws_access_key_id'],\n",
    "        aws_secret_access_key=extra['aws_secret_access_key']\n",
    "    )\n",
    "    \n",
    "    s3.upload_file('/local/path/file.txt', 'bucket-name', 'file.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e374b0-3430-4a9d-8c2e-aaa68b1cda4c",
   "metadata": {},
   "source": [
    "# Подключение к Minio через библиотеку Minio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b56e1bd-7a84-4148-babc-e89c68855023",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Minio(\n",
    "    endpoint=os.getenv(\"MINIO_ENDPOINT\"), # для airflow нужно будет переписать чтобы значения брались из Variables\n",
    "    access_key=os.getenv(\"MINIO_ACCESS_KEY\"),\n",
    "    secret_key=os.getenv(\"MINIO_SECRET_KEY\"),\n",
    "    secure=False  \n",
    ")\n",
    "\n",
    "# Проверяем список бакетов\n",
    "try:\n",
    "    buckets = client.list_buckets()\n",
    "    print(\"Подключение к MinIO через MinIO успешно. Список бакетов:\")\n",
    "    for b in buckets:\n",
    "        print(f' - {b.name}')\n",
    "except Exception as e:\n",
    "    print(\"Ошибка подключения к MinIO:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f418fbc2-8e36-4700-93fd-b1fcc2587642",
   "metadata": {},
   "source": [
    "# Проверка SparkSession "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf8eb5c-412d-4314-8517-dbc3e853b605",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MinIO Test\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.range(10)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abad1ce4-3e3d-4c6a-a330-7b59e312f1d6",
   "metadata": {},
   "source": [
    "# Запуск SparkSession с различными параметрами "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053e4401-70af-4e95-aef3-d3b55b2aee7f",
   "metadata": {},
   "source": [
    "**Имя приложения, которое будет отражено в Spark UI и логах**\n",
    "\n",
    "appName(\"SparkProject\")\n",
    "\n",
    "**Локальный режим (может быть yarn, mesos, kubernetes)**\n",
    "\n",
    "master(\"local[*]\") \n",
    "\n",
    "**Количество партиций, используемых при операциях shuffle (например, groupBy, join). По умолчанию = 200.\n",
    "Для небольших наборов данных можно уменьшить, чтобы не было лишних партиций.**\n",
    "\n",
    "config(\"spark.sql.shuffle.partitions\", \"200\") \n",
    "\n",
    "**Объем памяти, выделяемый каждому executor'у. Чем больше данных — тем больше памяти потребуется.**\n",
    "\n",
    "config(\"spark.executor.memory\", \"4g\")\n",
    "\n",
    "**Объем памяти для драйвера — основной управляющий процесс, запускающий задачи и отслеживающий их выполнение.**\n",
    "\n",
    "config(\"spark.driver.memory\", \"2g\") \n",
    "\n",
    "**Количество ядер CPU, которые каждый executor может использовать. Больше ядер — больше параллелизма, но стоит учитывать общую нагрузку.**\n",
    "\n",
    "config(\"spark.executor.cores\", \"4\")\n",
    "\n",
    "**Включает динамическое выделение executors в зависимости от загрузки. Позволяет Spark автоматически масштабироваться: добавлять и убирать executors в зависимости от нужд приложения. Требует настройки shuffle service.**\n",
    "\n",
    "config(\"spark.dynamicAllocation.enabled\", \"true\")\n",
    "\n",
    "**Указывает директорию для checkpoint'ов (резервного сохранения состояния, особенно в streaming-приложениях).**\n",
    "\n",
    "config(\"spark.checkpoint.dir\", \"/path/to/checkpoint/dir\")\n",
    "\n",
    "**Путь к директории, где Spark SQL будет хранить управляемые таблицы (при использовании saveAsTable, например).**\n",
    "\n",
    "config(\"spark.sql.catalogImplementation\", \"hive\")\n",
    "\n",
    "\n",
    "**Указывает, что Spark должен использовать Hive Catalog (а не встроенный In-Memory метастор). Требует наличие Hive или его имитации (например, Derby DB).**\n",
    "\n",
    ".enableHiveSupport()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4f7ec5-ab03-4eb3-8bb7-00ebd837675b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"SparkProject\") \\ \n",
    "#     .master(\"local[*]\") \\ \n",
    "#     .config(\"spark.sql.shuffle.partitions\", \"200\") \\ \n",
    "#     .config(\"spark.executor.memory\", \"4g\") \\ \n",
    "#     .config(\"spark.driver.memory\", \"2g\") \\ \n",
    "#     .config(\"spark.executor.cores\", \"4\") \\ \n",
    "#     .config(\"spark.dynamicAllocation.enabled\", \"true\") \\ \n",
    "#     .config(\"spark.checkpoint.dir\", \"/path/to/checkpoint/dir\") \\ \n",
    "#     .config(\"spark.sql.warehouse.dir\", \"/path/to/warehouse/dir\") \\ \n",
    "#     .config(\"spark.sql.catalogImplementation\", \"hive\") \\ \n",
    "#     .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03561232-96d9-4476-81fd-d6525024dbb3",
   "metadata": {},
   "source": [
    "# Получаем данные по API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbe37b9-b575-4a41-8f97-5795624187d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_api_data(env_name):\n",
    "    load_dotenv()\n",
    "    # Получаем ссылку из файла .env\n",
    "    api_url = os.getenv(env_name) # для airflow нужно будет переписать чтобы значения брались из Variables\n",
    "    # Проверка наличия ссылки\n",
    "    if not api_url:\n",
    "        logging.error(f'Указанный URL не задан в файле .env') # для airflow нужно будет переписать чтобы значения брались из Variables\n",
    "        return None\n",
    "    try:\n",
    "        response = requests.get(api_url)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            json_str = json.dumps(data, ensure_ascii=False, indent=2)\n",
    "            logging.info('Данные успешно загружены')\n",
    "            return json_str\n",
    "        else:\n",
    "            logging.error(f'Error: {response.status_code} - Данные не загружены')\n",
    "            return None\n",
    "    except:\n",
    "        logging.error(f'Error{e} - Данные не загружены')\n",
    "        raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6287e45e-c426-4b26-8c1e-6ac79a4a37c2",
   "metadata": {},
   "source": [
    "# Сохраняем JSON в MinIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e36b1b0-3a4f-48f6-920a-dd8fbe86a51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json_to_minio(env_name):\n",
    "    data = get_api_data(env_name)\n",
    "    \n",
    "# Написать функцию, чтобы записать в Minio JSON\n",
    "# Проверить в какой бакет пишем перезаписью\n",
    "# Через Spark читать файл\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a295514d-7ef2-47b5-b48f-9a439039b6c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390dfedf-7d9d-418c-9f1a-6fb6ba2c8937",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkProject\") \\ \n",
    "    .master(\"local[*]\") \\ \n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\ \n",
    "    .config(\"spark.driver.memory\", \"2g\") \\ \n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\") \\ \n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5a9986-9252-4f28-8a73-972086cd03b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d27737-e50e-4100-aa07-bc9071ca778b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fb8478-b0db-4296-b5cb-f9a357a1a203",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3879c2d3-a701-4b42-873b-e5431454b6e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d888304-470e-4530-8a9c-fe1662a929b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
